{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwavA8kBRd0U"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabV3_ResNet50_Weights\n",
        "import torchvision.models as models\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import gc\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "import os\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "from skimage import measure\n",
        "from shapely.geometry import Polygon, mapping\n",
        "from osgeo import gdal, gdalconst\n",
        "import glob\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio geojson\n",
        "import geojson\n",
        "import rasterio\n",
        "from rasterio.transform import Affine"
      ],
      "metadata": {
        "id": "TdCZHRpMRsKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If using google colab:"
      ],
      "metadata": {
        "id": "am50ZBi2bGUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Sl-osU2DbAP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model (DeeplabV3)"
      ],
      "metadata": {
        "id": "DslIoNbmSggt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Deeplabv3(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.deeplab = models.segmentation.deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
        "        # One class(forest/non-forest)\n",
        "        self.deeplab.classifier[4] = nn.Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.deeplab(x)['out']\n",
        "        out = self.out(x1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "-4zeByfkSZ4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "- Our data: 70 tiles from Cantabria, 15000x15000pixels each\n",
        "- We are going to use the model as a moving camera that slides over the large tile."
      ],
      "metadata": {
        "id": "Y9T0p6VLTM-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_export(model = Deeplabv3(), model_path=None, input_img_path=None,output_mask_dir=None, final_mask_name = None, simple_inference_mode = True):\n",
        "  \"\"\"\n",
        "  - Perform inference on images, and export the prediction masks.\n",
        "  Args:\n",
        "      model (Deeplabv3 object)\n",
        "      model_path (str): path to Deeplabv3 model weights\n",
        "      input_img_path (str): path to input images\n",
        "      output_mask_dir (str): dir to save the prediction mask\n",
        "      final_mask_name (str): name for the prediction mask, remember to specify the format (.png, .jpg, etc.)\n",
        "      simple_inference_mode (bool): True by default. Do not set it to false unless you are familiar with our project.\n",
        "  Returns:\n",
        "      None (This function will export the prediction masks to the specified directory)\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the model\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  # Our models are stored in a .pt file as a dictionary, the weights can be found under the \"model_state_dict\" key. Change this section if using a different format.\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device)[\"model_state_dict\"])\n",
        "  model.to(device).eval()\n",
        "\n",
        "\n",
        "  if simple_inference_mode:\n",
        "    # Input image\n",
        "    input_img = cv2.imread(input_img_path)/255.0\n",
        "    original_size = input_img.shape[:2]\n",
        "    input_img = cv2.resize(input_img,(512,512))\n",
        "    input_tensor = torch.tensor(np.transpose(input_img, (2,0,1)),dtype=torch.float32).unsqueeze(axis=0).to(device)\n",
        "\n",
        "    # Inference\n",
        "    output = model(input_tensor)\n",
        "    output = np.transpose(output.cpu().detach().numpy()[-1], (1, 2, 0))\n",
        "    # Final mask\n",
        "    final_output = np.expand_dims(cv2.resize(output, original_size, interpolation=cv2.INTER_LINEAR), axis=-1)\n",
        "    final_output = (np.where(final_output>0.5,1,0)*255).astype(np.uint8)\n",
        "  else:\n",
        "    input_img = cv2.imread(input_img_path)\n",
        "    # Calculate the x and y difference for padding\n",
        "    window_size = 1000 # Size of the sliding window\n",
        "    dif_y = (np.round(input_img.shape[0]/window_size)*window_size-input_img.shape[0]).astype(int)\n",
        "    dif_x = (np.round(input_img.shape[1]/window_size)*window_size-input_img.shape[1]).astype(int)\n",
        "\n",
        "    # Make a grid\n",
        "    x_list = np.array([i for i in range(np.round(input_img.shape[0]/window_size).astype(int))])\n",
        "    y_list = np.array([i for i in range(np.round(input_img.shape[1]/window_size).astype(int))])\n",
        "    X, Y = np.meshgrid(x_list,y_list)\n",
        "    XY = np.stack((X, Y), axis=-1).flatten().reshape(-1, 2)\n",
        "\n",
        "    # Create a bigger matrix to place the input image in the center\n",
        "    padding = 256\n",
        "    matrix_ones = np.ones((input_img.shape[0]+padding*2+dif_y, input_img.shape[1]+padding*2+dif_x, 3))*255.0  # Create a \"ones\" matrix with a bigger size than the input image (-256, 256)\n",
        "    matrix_ones[padding:padding+input_img.shape[0], padding:padding+input_img.shape[1], :] = input_img  # Place the input image in the center of the matrix\n",
        "    matrix_zeros_output_list = []\n",
        "\n",
        "    # Inference setup: For each tile, perform 5 inferences, 4 at the corners, 1 at the center\n",
        "    q_list = [[1,1,1,1],[0,0,0,0],[2,2,0,0],[0,0,2,2],[2,2,2,2]]\n",
        "\n",
        "    for q_idx, q in enumerate(q_list):\n",
        "      matrix_zeros_output = np.zeros((input_img.shape[0]+padding*2+dif_y, input_img.shape[1]+padding*2+dif_x, 1)) # Create a zeros matrix with a bigger size than the input image (-256, 256)\n",
        "      for idx, (x0, y0) in enumerate(XY):\n",
        "        img_pos = np.array([x0, y0])\n",
        "\n",
        "        x_start = img_pos[0]*window_size + q_list[q_idx][0]*padding\n",
        "        x_end = (img_pos[0]+1)*window_size + q_list[q_idx][1]*padding\n",
        "        y_start = img_pos[1]*window_size + q_list[q_idx][2]*padding\n",
        "        y_end = (img_pos[1]+1)*window_size + q_list[q_idx][3]*padding\n",
        "\n",
        "        # Print progress\n",
        "        if idx%50==0:\n",
        "          print(f\"Processing image position: {idx+1}/{XY.shape[0]} - ({x0}, {y0})\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        input_img_slice = matrix_ones[x_start:x_end, y_start:y_end,:]\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        input_img_slice = cv2.resize(input_img_slice, (512, 512))/255.0\n",
        "        # Transpose and convert to tensor\n",
        "        input_img_slice = torch.tensor(np.transpose(input_img_slice, (2, 0, 1)), dtype=torch.float32)\n",
        "        # Add batch dimension and move to device\n",
        "        input_img_slice = input_img_slice.unsqueeze(0).to(device)\n",
        "        # Prediction\n",
        "        output = model(input_img_slice)\n",
        "        output = np.transpose(output.cpu().detach().numpy()[-1], (1, 2, 0))\n",
        "        output = np.expand_dims(cv2.resize(output, (window_size, window_size), interpolation=cv2.INTER_LINEAR), axis=-1)\n",
        "        matrix_zeros_output[x_start:x_end, y_start:y_end,:] += output\n",
        "      matrix_zeros_output_list.append(matrix_zeros_output)\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "\n",
        "    # normalization\n",
        "    avg_output = np.sum(np.array(matrix_zeros_output_list),axis=0)\n",
        "    avg_output[padding:(padding+input_img.shape[0]+dif_x),padding:(padding+input_img.shape[1]+dif_y),:] /= 2\n",
        "    avg_output[:,padding*2:(input_img.shape[1]+dif_y),:] *=(2/3)\n",
        "    avg_output[padding*2:(input_img.shape[0]+dif_x),:,:] *=(2/3)\n",
        "    avg_output[padding*2:(input_img.shape[0]+dif_x),padding*2:(input_img.shape[1]+dif_y),:] *=(9/10)\n",
        "\n",
        "    # Final mask\n",
        "    final_output = avg_output[padding:(padding+input_img.shape[0]), padding:(padding+input_img.shape[1]), :]\n",
        "    max = final_output.max()\n",
        "    final_output = (final_output/max*255).astype(np.uint8)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "  # Export\n",
        "  cv2.imwrite(Path(output_mask_dir) / final_mask_name,final_output)\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        ""
      ],
      "metadata": {
        "id": "5YzKbl1QSoMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "input_img_dir = \"/content/drive/MyDrive/ML/Cantabria/Cantabria_3000x3000\"\n",
        "model_path_list = glob.glob(\"/content/drive/MyDrive/ML/Weights/Loveda_Jordan_Models/*\")\n",
        "output_mask_dir = \"/content/drive/MyDrive/ML/Masks/Loveda_multi_mask/13\""
      ],
      "metadata": {
        "id": "9rjXCEBNlQHr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_list = [13,18,32]\n",
        "for model_path in model_path_list:\n",
        "  epoch = model_path.split(\"_\")[-1].split(\".\")[0]\n",
        "  output_mask_path = output_mask_dir + \"/\" + epoch\n",
        "  for root,_,filenames in os.walk(input_img_dir):\n",
        "      for filename in filenames:\n",
        "        if filename.endswith(\".png\"):\n",
        "          print(f\"image: {filename}\")\n",
        "          predict_export(model = Deeplabv3(),\n",
        "              model_path = model_path,\n",
        "              input_img_path=os.path.join(input_img_dir,filename),\n",
        "              output_mask_dir=output_mask_dir,\n",
        "              final_mask_name = filename.split(\".\")[0] + \".tif\",\n",
        "              simple_inference_mode = False)"
      ],
      "metadata": {
        "id": "e8Fc1w-jpf9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Georeferencing"
      ],
      "metadata": {
        "id": "zO1FMBJZwbuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_path_list = glob.glob(\"/content/drive/MyDrive/ML/Masks/Loveda_multi_mask/final/*.tif\")\n",
        "pgw_path_list = glob.glob(\"/content/drive/MyDrive/ML/Cantabria/Cantabria_3000x3000/*.pgw\")\n",
        "def get_transform_from_pgw(pgw_path):\n",
        "    with open(pgw_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        pixel_size_x = float(lines[0].strip())\n",
        "        rotation_x = float(lines[1].strip())\n",
        "        rotation_y = float(lines[2].strip())\n",
        "        pixel_size_y = float(lines[3].strip())\n",
        "        top_left_x = float(lines[4].strip())\n",
        "        top_left_y = float(lines[5].strip())\n",
        "    return (top_left_x, pixel_size_x, rotation_x, top_left_y, rotation_y, pixel_size_y)\n",
        "\n",
        "for idx, pgw_path in enumerate(pgw_path_list):\n",
        "  gt = get_transform_from_pgw(pgw_path)\n",
        "  # Set georeferenced info\n",
        "  ds2 = gdal.Open(mask_path_list[idx])\n",
        "  ds2.SetGeoTransform(gt)\n",
        "  ds2.FlushCache()\n",
        "  ds2 = None  # Close the dataset to save changes\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "kyhWWrDywfB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversion to vector"
      ],
      "metadata": {
        "id": "lteo-Dq0wo31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "import numpy as np\n",
        "from shapely.geometry import Polygon, mapping, LinearRing\n",
        "import geojson\n",
        "from rasterio.transform import Affine\n",
        "from pathlib import Path\n",
        "from shapely.ops import unary_union\n",
        "from shapely.prepared import prep\n",
        "os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = pow(2,40).__str__()\n",
        "import cv2 # import after setting OPENCV_IO_MAX_IMAGE_PIXELS\n",
        "\n",
        "def find_contours(img_tif_path,output_path):\n",
        "  idx = img_tif_path.split(\"_\")[-1].split(\".\")[0]\n",
        "  epoch = img_tif_path.split(\"_\")[-3]\n",
        "  # Load tiff\n",
        "  with rasterio.open(img_tif_path) as src:\n",
        "    mask = src.read(1)  # read first band\n",
        "    transform = src.transform  # Affine transform: maps pixel -> world coords\n",
        "\n",
        "  # Check if mask is binary (0 or 1)\n",
        "  binary_mask = (mask > 0).astype(np.uint8) * 255  # values 0 or 255\n",
        "\n",
        "  # Find contours\n",
        "  contours, hierarchy = cv2.findContours(binary_mask, cv2.RETR_CCOMP , cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  polygons = []\n",
        "  used_as_hole = set()\n",
        "  for i, contour in enumerate(contours):\n",
        "    if hierarchy[0][i][3] == -1:  # No parent: it's an outer contour\n",
        "        contour = contour.squeeze()  # shape: (N, 2)\n",
        "        if len(contour.shape) != 2 or contour.shape[0] < 3:\n",
        "            continue\n",
        "        shell = [rasterio.transform.xy(transform, y, x, offset=\"center\") for x,y in contour]\n",
        "        shell_poly = Polygon(shell)\n",
        "        prepared_shell = prep(shell_poly)\n",
        "\n",
        "        holes = []\n",
        "        for j, child in enumerate(contours):\n",
        "            if hierarchy[0][j][3] == i:  # Child of contour i\n",
        "                child = child.squeeze()\n",
        "                hole_coords = [rasterio.transform.xy(transform, y, x, offset=\"center\") for x,y in child]\n",
        "                hole_ring = LinearRing(hole_coords)\n",
        "\n",
        "                if prepared_shell.contains(Polygon(hole_ring)):\n",
        "                    holes.append(hole_coords)\n",
        "                    used_as_hole.add(j)\n",
        "\n",
        "        polygon = Polygon(shell, holes)\n",
        "        polygons.append(polygon)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  gdf = gpd.GeoDataFrame({\"geometry\": polygons},crs=25830)\n",
        "  gdf.to_crs(epsg=4326, inplace=True)  # Set the coordinate reference system to WGS84\n",
        "  gdf.to_csv(os.path.join(output_path,f\"deeplabv3_loveda_epoch{epoch}_output{idx}.csv\"))\n",
        "\n",
        "coordinates_path = \"/content/drive/MyDrive/ML/Coordinates/loveda_final\"\n",
        "for mask_path in mask_path_list:\n",
        "    find_contours(mask_path,coordinates_path)"
      ],
      "metadata": {
        "id": "ie6LhDybwrHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unify tables"
      ],
      "metadata": {
        "id": "gu64gEduxjE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"/content/drive/MyDrive/ML/Coordinates/loveda_final\"\n",
        "path_list = glob.glob(str(Path(root)/\"*\"),recursive=True)\n",
        "name_list = [path.split(\"output\")[-1].split(\".\")[0] for path in path_list]\n",
        "df_list = []\n",
        "for idx,path in enumerate(path_list):\n",
        "  new_df = pd.read_csv(path)\n",
        "  new_df.iloc[:,0]=np.int8(name_list[idx])\n",
        "  df_list.append(new_df)\n",
        "df_final = pd.concat(df_list,ignore_index=True)\n",
        "df_final.rename(columns={\"Unnamed: 0\": \"id\"}, inplace=True)\n",
        "df_final.head()"
      ],
      "metadata": {
        "id": "6R0CAKZpw3ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.to_csv(Path(root)/\"final.csv\")\n",
        "print(Path(root)/\"loveda_final.csv\")"
      ],
      "metadata": {
        "id": "iaSq0b5bxiCN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}